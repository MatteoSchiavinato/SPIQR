#!/usr/bin/env python

### new version!! ###
### implemented the storage of the best quality read ###

### WARNING ###
	# USE THIS PROGRAM WITH PYTHON 2.7.*
	# Matteo Schiavinato
# 2016
# BOKU - Vienna Institute of Biotechnology


import time
import sys
import os
import argparse
import numpy 
import itertools

#
#parser
parser = argparse.ArgumentParser(prog="SPIQR deduplicate", formatter_class=argparse.RawDescriptionHelpFormatter, description='''
----------------------------------------------------------------------------
### Matteo Schiavinato # 2016 # BOKU - Vienna Institute of Biotechnology ###
----------------------------------------------------------------------------
'''
, usage="\n\n\nSPIQR deduplicate [options]\n\n\n")
parser.add_argument("-t", "--reads-type", help="PE: paired end, SE: single end.", required=True)
parser.add_argument("-R1", "--reads-1", help="The FASTQ reads file to be processed. In case of paired end reads, the first of the two FASTQ reads files. [mandatory]", metavar="FILE", required=True)
parser.add_argument("-R2", "--reads-2", help="Optional: if paired-end, the second of the two FASTQ reads files. Not needed if single-end.", metavar="FILE")
parser.add_argument("--fasta-out", help="Force FASTA output (removing quality scores).", action="store_true")
parser.add_argument("-o", "--output-directory", help="The directory where to write the output files. Defaults to WORKDIR.", metavar="PATH", default=".")
parser.add_argument("-l", "--read-length", help="OPTIONAL: Specify how long are your Illuina reads. If not provided, the program will deduce this data from the reads theirselves. Speeds up things.", metavar="INTEGER", type=int)
parser.add_argument("-r", "--region-of-analysis", help="OPTIONAL: How long do you want the tested region of identity to be? The program will consider a region with a spanning length equal to [N] percentage of the read length.\nIf not provided, the program will automatically use the 99-percent of a read length.", metavar="INTEGER", default=99, type=int)
parser.add_argument("-x", "--region-start", help="Consider a particular region of the reads for the deduplication process. The region will start from this position and end in the [-y | --region-end] position.", metavar="INTEGER", type=int)
parser.add_argument("-y", "--region-end", help="Consider a particular region of the reads for the deduplication process. The region will start from the [-x | --region-start] position and end in this position.", metavar="INTEGER", type=int)
args = parser.parse_args()
#
sys.stderr.write("[{0}]\tBegin\n".format(time.asctime()))
#
# preliminary calculations
# script verifies the read length
# estimates the region of analysis
# detects how many nucleotides will be skipped from the margins
typeslist = ["PE", "SE"]
sys.stderr.write("[{0}]\tDoing verifications\n".format(time.asctime()))
if args.reads_type not in typeslist:
	sys.exit("Reads type must be one between PE or SE. See help for instructions.\n")
	#
if not args.read_length:
	k=3
	INPUT = open(args.reads_1, "r")
	total = 0
	for line in INPUT:
		x = line.rstrip()
		k+=1
		if k%4 == 1:
			total += len(x)
		if k == 1003:
			break
	#
	INPUT.close()
	read_length = int(float(total) / float(k/4))
	#
else:
	read_length = args.read_length
	#
sys.stderr.write("\t- Estimated a read length of {0} nucleotides on average.\n".format(read_length))
#
if args.region_start:
	del args.region_of_analysis

if (args.region_start and not args.region_end) or (args.region_end and not args.region_start):
	sys.exit('''
You must specify both [-x | --region-start] and [-y | --region-end].\n
''')

if not args.region_start:
	region = int((float(read_length) / 100)*args.region_of_analysis)
	n = int((float(read_length) / 100)*args.region_of_analysis)
	#
	sys.stderr.write("\t- Estimated a region of analysis of {0} nucleotides.\n".format(region))
	dist_from_bord = (read_length - region)/2
	sys.stderr.write("\t- {0} nucleotides from each margin of the read will be skipped. The central part will be processed.\n".format(dist_from_bord))
	start = int(dist_from_bord) + 1
	end = read_length - int(dist_from_bord)
	ratio = float(args.region_of_analysis) / 2
	sys.stderr.write("\t- Reads will be processed using the {0}% of their sequence, {1}% on the left and {1}% on the right of the center of the read.\n".format(args.region_of_analysis, ratio))
	#
elif args.region_start:
	start = int(args.region_start)
	end = int(args.region_end)
	sys.stderr.write("\t- Reads will be processed using the region going from {0} to {1}\n".format(start, end))
	#

# set extension for output files
extension = "fastq"
if args.fasta_out:
	extension = "fa"

#function definition
def make_unique(sequences_list):
    seen = set()
    seen_add = seen.add
    return [x for x in sequences_list if not (x in seen or seen_add(x))]

# generating main dictionary
# without duplicates
# keeping only the best read in quality among the duplicates
if args.reads_type == "SE":
	QUALS = {}
	filename = args.reads_1.split("/")[-1]
	sys.stderr.write("[{0}]\tReading Reads File {1}\n".format(time.asctime(), filename))
	INPUT = open(args.reads_1, "r")
	counter = 3
	k = 0
	DISCARDED = open("{0}/discarded.{1}".format(args.output_directory, filename), "w")
	for line in INPUT:
		counter += 1
		if counter%4 == 0:
			k += 1
			read_name = line.rstrip()
			if k == 10000000:
				sys.stderr.write("[{0}]\tProcessed {1} reads\n".format(time.asctime(), k))
		elif counter%4 == 1:
			read_sequence = line.rstrip()
			short_x = read_sequence[start:end]
			try:
				QUALS[short_x]
			except KeyError:
				QUALS[short_x] = ["", "", "", "", 0]
		elif counter%4 == 2:
			read_separator = line.rstrip()
		elif counter%4 == 3:
			read_quality = line.rstrip()
			quality_score = int(numpy.frombuffer(read_quality, "uint8").sum())
			if (quality_score > QUALS[short_x][4]) and (QUALS[short_x][4] > 0):
				if args.fasta_out:
					print >>DISCARDED, "\n".join(QUALS[short_x][0:2])
				else:
					print >>DISCARDED, "\n".join(QUALS[short_x][0:4])
				QUALS[short_x] = [read_name, read_sequence, read_separator, read_quality, quality_score]
			else:
				if args.fasta_out:
                                        print >>DISCARDED, "\n".join([read_name, read_sequence])
                                else:
                                        print >>DISCARDED, "\n".join([read_name, read_sequence, read_separator, read_quality])
	#
	INPUT.close()
	DISCARDED.close()
	sys.stderr.write("[{0}]\tAnalyzed {1} reads\n".format(time.asctime(), k))
	ratio = round(float(float(len(QUALS.keys())) / float(k))*100, 1)
	sys.stderr.write("[{0}]\tOf these, {1} survived deduplication ( {2} % )\n".format(time.asctime(), len(QUALS.keys()), ratio))
	sys.stderr.write("[{0}]\tDuplicated reads stored in {1}/discarded.{2}\n".format(time.asctime(), args.output_directory, filename))
	#
	sys.stderr.write("[{0}]\tWriting to output\n".format(time.asctime()))
	OUTPUT = open("{0}/dedup.{1}".format(args.output_directory, filename), "w")
	for seed in QUALS:
		del QUALS[seed][4]
		if args.fasta_out:
			QUALS[seed] = QUALS[seed][0:2]
			QUALS[seed][0] = ">" + QUALS[seed][0][1:]
		print >>OUTPUT, "\n".join(QUALS[seed])
	#
	OUTPUT.close()
	sys.stderr.write("[{0}]\tWrote to output. Check results at: {1}/dedup.{2}\n".format(time.asctime(), args.output_directory, filename))
#
elif args.reads_type == "PE":
	QUALS = {}
	filename_1 = args.reads_1.split("/")[-1]
	filename_2 = args.reads_2.split("/")[-1]
	DISCARDED_1 = open("{0}/discarded.{1}".format(args.output_directory, filename_1), "w")
	DISCARDED_2 = open("{0}/discarded.{1}".format(args.output_directory, filename_2), "w")
        sys.stderr.write("[{0}]\tReading Reads Files {1} and {2}\n".format(time.asctime(), filename_1, filename_2))
        with open(args.reads_1) as INPUT_1:
                with open(args.reads_2) as INPUT_2:
                        counter = 3
                        k=0
                        for line_x, line_y in itertools.izip(INPUT_1, INPUT_2):
				counter += 1
				if counter%4 == 0:
					read_name_x = line_x.rstrip()
					read_name_y = line_y.rstrip()
                                elif counter%4 == 1:
					read_sequence_x = line_x.rstrip()
					read_sequence_y = line_y.rstrip()
                                        short_x = read_sequence_x[start:end]
                                        short_y = read_sequence_y[start:end]
                                        short_read = short_x + "%" + short_y
					try:
						QUALS[short_read]
					except KeyError:
						QUALS[short_read] = [[], [], 0]
                                        k+=1
					if k%10000000 == 0:
						sys.stderr.write("[{0}]\tProcessed {1} mates\n".format(time.asctime(), k))
				elif counter%4 == 2:
					read_separator_x = line_x.rstrip()
					read_separator_y = line_y.rstrip()
				elif counter%4 == 3:
					read_quality_x = line_x.rstrip()
					quality_score_x = int(numpy.frombuffer(read_quality_x, "uint8").sum())
					read_quality_y = line_y.rstrip()
					quality_score_y = int(numpy.frombuffer(read_quality_y, "uint8").sum())
					quality_score = quality_score_x + quality_score_y
					if (quality_score > QUALS[short_read][2]) and (QUALS[short_read][2] > 0):
						if args.fasta_out:
							print >>DISCARDED_1, "\n".join(QUALS[short_read][0][0:2])
							print >>DISCARDED_2, "\n".join(QUALS[short_read][1][0:2])
						else:
							print >>DISCARDED_1, "\n".join(QUALS[short_read][0][0:4])
							print >>DISCARDED_2, "\n".join(QUALS[short_read][1][0:4])
						QUALS[short_read] = [[read_name_x, read_sequence_x, read_separator_x, read_quality_x], [read_name_y, read_sequence_y, read_separator_y, read_quality_y], quality_score]
					else:
						if args.fasta_out:
                                                        print >>DISCARDED_1, "\n".join([read_name_x, read_sequence_x])
                                                        print >>DISCARDED_2, "\n".join([read_name_y, read_sequence_y])
                                                else:
                                                        print >>DISCARDED_1, "\n".join([read_name_x, read_sequence_x, read_separator_x, read_quality_x])
                                                        print >>DISCARDED_2, "\n".join([read_name_y, read_sequence_y, read_separator_y, read_quality_y])
	#
        INPUT_1.close()
        INPUT_2.close()
	DISCARDED_1.close()
	DISCARDED_2.close()
	#
	sys.stderr.write("[{0}]\tAnalyzed {1} pairs ( {2} reads )\n".format(time.asctime(), k, k*2))
        ratio = round(float(float(len(QUALS.keys())) /float(k))*100, 1)
        sys.stderr.write("[{0}]\tOf these, {1} pairs survived deduplication ( {2} % )\n".format(time.asctime(), len(QUALS.keys()), ratio))
	sys.stderr.write("[{0}]\tStored discarded pairs in {1}/discarded.{2} and {1}/discarded.{3}\n".format(time.asctime(), args.output_directory, filename_1, filename_2))
	#
	sys.stderr.write("[{0}]\tWriting to output\n".format(time.asctime()))
        OUTPUT_1 = open("{0}/dedup.{1}".format(args.output_directory, filename_1), "w")
	OUTPUT_2 = open("{0}/dedup.{1}".format(args.output_directory, filename_2), "w")
        for seed in QUALS:
		if args.fasta_out:
			QUALS[seed][0] = QUALS[seed][0][0:2]
			QUALS[seed][0][0] = ">" + QUALS[seed][0][0][1:]
			QUALS[seed][1] = QUALS[seed][1][0:2]
			QUALS[seed][1][0] = ">" + QUALS[seed][1][0][1:]
                print >>OUTPUT_1, "\n".join(QUALS[seed][0])
		print >>OUTPUT_2, "\n".join(QUALS[seed][1])
        #
        OUTPUT_1.close()
	OUTPUT_2.close()
        sys.stderr.write("[{0}]\tWrote to output. Check results at: {1}/dedup.{2} and {1}/dedup.{3}\n".format(time.asctime(), args.output_directory, filename_1, filename_2))
